<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="css/styles.css" />
  <title>Data Science</title>
</head>

<body>
  <div class="heading">
    <h1 class="comfort">Data Science Materials</h1>
  </div>
  <div class="linkref">
    <li class="links"><a href='https://towardsdatascience.com/' target="_blank">Towards Data Science</a></li>

    <li class="links"><a href="https://docs.anaconda.com/free/miniconda/" target="_blank">Miniconda</a></li>

    <li class="links"><a href="https://www.python.org/" target="_blank">Python</a></li>
  </div>
  <div class="maincontent">
    <h2>What is Multicollinearity</h2>
    <p>
      Multicollinearity occurs when two or more independent variables in a
      regression model are highly correlated with each other. This means that
      they share a lot of information, and it becomes difficult to isolate the
      individual effect of each variable on the dependent variable.
    </p>
    <h2>How does VIF work?</h2>
    <p>
      The VIF score for a particular independent variable is calculated by
      regressing that variable on all the other independent variables in the
      model. The VIF score is then equal to the reciprocal of 1 minus the
      R-squared value of this regression. In simpler terms, it tells you how
      much the variance of the coefficient estimate for that variable is
      inflated due to multicollinearity.
      <b>Higher VIF scores indicate a greater degree of multicollinearity</b>.
      While there is no universally accepted threshold for what constitutes a
      "high" VIF score, a common rule of thumb is that values above 5 or 10
      suggest potential problems.
    </p>
    <h2>Why is VIF important?</h2>
    <p>
      Multicollinearity can lead to several problems in regression analysis,
      including:
    </p>
    <ul>
      <li>
        <b>Inaccurate coefficient estimates:</b> The coefficients of the
        independent variables may be biased and unreliable, making it
        difficult to interpret their meaning.
      </li>
      <li>
        <b>Increased standard errors:</b> The standard errors of the
        coefficients will be inflated, making it harder to assess the
        statistical significance of the results.
      </li>
      <li>
        <b>Unstable model:</b> The model may be unstable and change
        significantly with small changes in the data.
      </li>
    </ul>
    <p>
      By using VIF scores, you can identify variables that are contributing to
      multicollinearity and take steps to address the issue. This may involve
      removing highly correlated variables from the model, combining them into
      a single variable, or collecting additional data.
    </p>
    <h2>What are the assumptions of Linear Regression?</h2>
    <p>
      Linear regression, though a powerful tool, relies on several key
      assumptions to ensure its validity and reliability. These assumptions
      essentially dictate the conditions under which the model produces
      accurate and interpretable results. Here are the main assumptions of
      linear regression:
    <ol type="1">
      <li><b>No Multicollinearity:</b> This means the independent variables in
        your model should not be highly correlated with each other. Imagine two
        variables providing almost the same information; it becomes tricky to
        disentangle their individual effects on the dependent variable.
        Multicollinearity can inflate standard errors and distort coefficient
        estimates.</li>
      <li><b>Homoscedasticity:</b> This assumption demands that the variance of
        the errors (residuals) around the regression line should be constant
        across all values of the independent variables. In other words, the
        "scatter" of the data points around the line shouldn't increase or
        decrease significantly with changes in the predictor variable.
        Violations of this assumption can affect the accuracy of hypothesis
        testing.</li>
      <li><b>Normality of Residuals:</b> Ideally, the errors (differences
        between predicted and actual values) should follow a normal
        distribution. This ensures that the model's predictions are
        statistically reliable. While not entirely strict, significant
        deviations from normality can impact the validity of inferences drawn
        from the model.</li>
      <li><b>Independence of Errors:</b> This assumption states that the errors
        associated with each observation are independent of each other. In other
        words, the error for one data point shouldn't influence the error for
        any other point. Dependence between errors can lead to inaccurate
        estimates of the model's variance and affect hypothesis testing.</li>
      <li><b>No Autocorrelation:</b> This assumption applies to time series
        data and demands that the errors for consecutive observations are not
        correlated. In simpler terms, the error at one time point shouldn't be
        "predictable" from the errors at previous time points. Autocorrelation
        can distort estimates of model parameters and affect the reliability of
        forecasts.</li>
    </ol>
    </p>
    <h2>What are the assumptions of Logistic Regression?</h2>
    <p>
      While similar to linear regression in some aspects, logistic regression carries its own set of assumptions:
    <ol type="1">
      <li><b>Binary Dependent Variable:</b> Unlike linear regression, which models continuous outcomes, logistic
        regression requires a binary dependent variable. This means it predicts the probability of an event occurring
        with only two possible outcomes, like "win" or "lose," "approved" or "rejected," etc.</li>
      <li><b>Independence of Observations:</b> Each observation in your dataset should be independent of all others.
        This means no repeated measurements or matched data, as dependence can influence the model's accuracy.</li>
      <li><b>No Multicollinearity:</b> Similar to linear regression, the independent variables shouldn't be highly
        correlated with each other. Multicollinearity can mislead the model and make it hard to interpret the individual
        effects of each variable.</li>
      <li><b>Linearity in the Logit:</b> This assumption focuses on the relationship between the independent variables
        and the logit transformation of the dependent variable's probability. While the actual relationship between
        variables can be nonlinear, this transformed relationship must be linear for the model to function properly.
      </li>
      <li><b>Large Enough Sample Size:</b> Logistic regression performs better with larger sample sizes, especially when
        the less frequent outcome has a lower probability. A general rule suggests at least 10 events per independent
        variable for the least frequent outcome. Smaller samples can lead to unreliable results.</li>
      <li><b>Absence of Extreme Outliers:</b> Outliers can significantly influence the model's predictions. It's
        essential to check for and potentially address extreme outliers that might distort the relationship between
        variables.</li>
    </ol>
    <b>Additional Notes:</b>
    While not a strict assumption, having a balanced dataset (roughly equal proportions of both outcomes) can improve
    model performance.
    Techniques like robust regression can sometimes mitigate the impact of minor violations of these assumptions.
    Remember, it's crucial to diagnose and address potential violations whenever possible to ensure the reliability and
    interpretability of your logistic regression model.
    </p>
    <h2>What is the difference between a classification model and a regression model</h2>
    <p>
      The key difference between a classification and a regression model lies in the type of output they predict:
    <p><b>Classification Models:</b></p>
    <ul>
      <li>Predict discrete categories or classes. Essentially, they assign data points to specific pre-defined groups.
      </li>
      <li>Examples: classifying emails as spam or not spam, recognizing handwritten digits, predicting whether a
        customer will churn.</li>
      <li>Output values are typically represented as integers or categorical labels.</li>
    </ul>
    <p><b>Regression Models:</b></p>
    <ul>
      <li>Predict continuous numerical values. They estimate a smooth relationship between the input variables and the
        output variable.</li>
      <li>Examples: predicting house prices, forecasting sales figures, estimating blood pressure based on various
        factors.</li>
      <li>Output values are continuous numbers on a specific scale.</li>
    </ul>
    <p><b>Additional Differences:</b></p>
    <ul>
      <li><b>Evaluation Metrics:</b> Different metrics are used to assess the performance of each model type. For
        classification, accuracy, precision, recall, and F1-score are commonly used. For regression, mean squared error
        (MSE), root mean squared error (RMSE), and R-squared are common metrics.</li>
      <li><b>Model Types:</b> Many different algorithms exist for both classification and regression. Some popular
        classification algorithms include decision trees, support vector machines (SVM), and k-nearest neighbors (kNN).
        Popular regression algorithms include linear regression, polynomial regression, and random forest regression.
      </li>
    </ul>
  </div>
</body>

</html>